# puppy/build_legal_db.py

"""
ë²•ë¥  ë¬¸ì„œ(txt, pdf)ë¥¼ ë¡œë“œí•˜ì—¬ 'êµ¬ì¡°ì  ì²­í‚¹(structural chunking)'ì„ ìˆ˜í–‰í•˜ê³ ,
HuggingFace ì„ë² ë”© ëª¨ë¸ì„ ì‚¬ìš©í•´ FAISS ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ìƒì„± ë° ì €ì¥í•©ë‹ˆë‹¤.

ì‹¤í–‰ ë°©ë²•:
python -m puppy.build_legal_db
"""

import os
import re
import pickle
from typing import List

import faiss
import numpy as np
from tqdm import tqdm

from langchain.schema import Document
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from .embedding import OpenAILongerThanContextEmb
from langchain_text_splitters import RecursiveCharacterTextSplitter
from dotenv import load_dotenv
load_dotenv(override=True)

# --- 1. ì„¤ì • (Configuration) ---
DATA_PATH = './data/legal_documents/'
DB_FAISS_PATH = "./vector_db/"
EMBEDDING_MODEL = "text-embedding-ada-002"

def load_documents(path: str) -> list[Document]:
    """ì§€ì •ëœ ê²½ë¡œì—ì„œ í…ìŠ¤íŠ¸ ë° PDF ë¬¸ì„œë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    print("ë¬¸ì„œ ë¡œë”© ì‹œì‘...")
    all_docs = []
    
    # ì§€ì •ëœ ê²½ë¡œì— ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ì˜¤ë¥˜ ë©”ì‹œì§€ì™€ í•¨ê»˜ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    if not os.path.isdir(path):
        print(f"ì˜¤ë¥˜: '{path}' ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return all_docs

    for filename in tqdm(os.listdir(path), desc="ë¬¸ì„œ ë¡œë”© ì¤‘"):
        full_path = os.path.join(path, filename)
        try:
            if filename.endswith('.pdf'):
                loader = PyPDFLoader(full_path)
            elif filename.endswith('.txt'):
                loader = TextLoader(full_path, encoding='utf-8')
            else:
                continue # ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì€ ê±´ë„ˆëœë‹ˆë‹¤.
            
            loaded_docs = loader.load()
            # ê° ë¬¸ì„œì— íŒŒì¼ëª…ì„ ë©”íƒ€ë°ì´í„°ë¡œ ì¶”ê°€
            for doc in loaded_docs:
                doc.metadata['source'] = filename
            all_docs.extend(loaded_docs)
        except Exception as e:
            print(f"'{filename}' íŒŒì¼ ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            
    print(f"ì´ {len(all_docs)}ê°œì˜ í˜ì´ì§€/íŒŒì¼ ë¡œë“œ ì™„ë£Œ.")
    return all_docs

def clean_text(text: str) -> str:
    """ë²•ë¥  í…ìŠ¤íŠ¸ì—ì„œ ë¶ˆí•„ìš”í•œ ê³µë°±, í˜ì´ì§€ ë²ˆí˜¸ ë“±ì„ ì •ì œí•©ë‹ˆë‹¤."""
    text = re.sub(r'-\s*\d+\s*-', '', text)  # í˜ì´ì§€ ë²ˆí˜¸ í˜•ì‹ (- 1 -) ì œê±°
    text = re.sub(r'\s{3,}', '\n\n', text).strip()  # ê³¼ë„í•œ ê³µë°±ì€ ë¬¸ë‹¨ìœ¼ë¡œ ë³€í™˜
    return text

def chunk_legal_documents(documents: list[Document]) -> list[Document]:
    """ë²•ë¥  ë¬¸ì„œì— ìµœì í™”ëœ 'êµ¬ì¡°ì  ì²­í‚¹'ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    print("êµ¬ì¡°ì  ì²­í‚¹ ì‹œì‘...")
    
    # ë²•ë¥  ì¡°í•­ êµ¬ì¡°ì— ê¸°ë°˜í•œ ë¶„ë¦¬ì ì„¤ì • (ì¡°/í•­/í˜¸)
    separators = [
        r'\n\nì œ[0-9]+ì¡°(?:ì˜[0-9]+)?\s*\(.+?\)',  # "ì œ1ì¡°(ëª©ì )" í˜•íƒœ
        r'\n\nì œ[0-9]+ì¡°(?:ì˜[0-9]+)?',             # "ì œ2ì¡°" í˜•íƒœ
        "\n\n", "\n", " ", ""
    ]
    
    # ëª¨ë“  ë¬¸ì„œë¥¼ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ ê²°í•©
    full_text = "\n\n".join([clean_text(doc.page_content) for doc in documents])
    
    text_splitter = RecursiveCharacterTextSplitter(
        separators=separators,
        chunk_size=1000,
        chunk_overlap=150,
        is_separator_regex=True,
        length_function=len,
    )
    
    chunks = text_splitter.split_text(full_text)
    
    doc_chunks = []
    for i, chunk_text in enumerate(tqdm(chunks, desc="ì²­í¬ ìƒì„± ì¤‘")):
        # ê° ì²­í¬ì˜ ì‹œì‘ ë¶€ë¶„ì—ì„œ 'ì œNì¡°' í˜•íƒœì˜ ì¡°í•­ ë²ˆí˜¸ë¥¼ ì¶”ì¶œí•˜ì—¬ ë©”íƒ€ë°ì´í„°ë¡œ í™œìš©
        match = re.search(r'^(ì œ[0-9]+ì¡°(?:ì˜[0-9]+)?)', chunk_text)
        article = match.group(1).strip() if match else f"ê¸°íƒ€ ì¡°í•­ Chunk {i+1}"
        
        # ê²€ìƒ‰ ì‹œ í™œìš©í•  ìˆ˜ ìˆë„ë¡ ê³ ìœ  IDì™€ ì¡°í•­ ì •ë³´ë¥¼ ë©”íƒ€ë°ì´í„°ì— í¬í•¨
        metadata = {"source": "ì¢…í•© ë²•ë¥  ë¬¸ì„œ", "article": article, "id": i}
        doc_chunks.append(Document(page_content=chunk_text, metadata=metadata))
        
    print(f"ì´ {len(doc_chunks)}ê°œì˜ ë²•ë¥  ì¡°í•­ ì²­í¬ ìƒì„± ì™„ë£Œ.")
    return doc_chunks

# â›”ï¸ ê¸°ì¡´ í•¨ìˆ˜ë¥¼ ì§€ìš°ê³ , ì•„ë˜ì˜ ìƒˆë¡œìš´ í•¨ìˆ˜ ì½”ë“œë¡œ ì™„ì „íˆ êµì²´í•´ì£¼ì„¸ìš”. â›”ï¸

def create_and_save_vector_db(chunks: list[Document]):
    """ì„ë² ë”© ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë²¡í„° DBë¥¼ ìƒì„±í•˜ê³  íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤. (ë©”ëª¨ë¦¬ ìµœì í™” ë²„ì „)"""
    print("ì„ë² ë”© ë° ë²¡í„° DB ìƒì„± ì‹œì‘...")

    embeddings_model = OpenAILongerThanContextEmb(
        embedding_model=EMBEDDING_MODEL,
        chunk_size=1000,
        verbose=False
    )

    # --- ğŸ‘‡ ì—¬ê¸°ê°€ í•µì‹¬ì ì¸ ìˆ˜ì • ë¶€ë¶„ì…ë‹ˆë‹¤! ---

    all_vectors = []
    batch_size = 8  # í•œ ë²ˆì— ì²˜ë¦¬í•  ì²­í¬ ê°œìˆ˜ (PC ì‚¬ì–‘ì— ë”°ë¼ ì¡°ì ˆ ê°€ëŠ¥)

    print(f"ì´ {len(chunks)}ê°œì˜ ì²­í¬ë¥¼ {batch_size}ê°œì”© ë‚˜ëˆ„ì–´ ì„ë² ë”©í•©ë‹ˆë‹¤.")

    # tqdmì„ ì‚¬ìš©í•˜ì—¬ ë°°ì¹˜ ì²˜ë¦¬ ì§„í–‰ ìƒí™©ì„ ì‹œê°ì ìœ¼ë¡œ í‘œì‹œ
    for i in tqdm(range(0, len(chunks), batch_size), desc="ë¬¸ì„œ ì„ë² ë”© ì¤‘"):
        batch_chunks = chunks[i:i + batch_size]
        batch_texts = [doc.page_content for doc in batch_chunks]

        # ì‹¤ì œ ì„ë² ë”© ì²˜ë¦¬
        batch_vectors = []
        for text in batch_texts:
            vector = embeddings_model(text)
            if hasattr(vector, 'shape') and len(vector.shape) == 2:
                vector = vector.flatten()
            batch_vectors.append(vector)
        all_vectors.extend(batch_vectors)

    print("ëª¨ë“  ë¬¸ì„œì˜ ì„ë² ë”©ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    # --- ğŸ‘† ì—¬ê¸°ê¹Œì§€ê°€ í•µì‹¬ì ì¸ ìˆ˜ì • ë¶€ë¶„ì…ë‹ˆë‹¤! ---

    embedding_dim = len(all_vectors[0])
    index = faiss.IndexFlatIP(embedding_dim)
    index = faiss.IndexIDMap2(index)

    ids = np.array([doc.metadata['id'] for doc in chunks])
    index.add_with_ids(np.array(all_vectors, dtype=np.float32), ids)

    os.makedirs(DB_FAISS_PATH, exist_ok=True)

    faiss_index_path = os.path.join(DB_FAISS_PATH, "legal_faiss.index")
    faiss.write_index(index, faiss_index_path)
    print(f"FAISS ì¸ë±ìŠ¤ê°€ '{faiss_index_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    doc_map = {doc.metadata['id']: doc for doc in chunks}
    docs_pkl_path = os.path.join(DB_FAISS_PATH, "legal_docs.pkl")
    with open(docs_pkl_path, "wb") as f:
        pickle.dump(doc_map, f)

    print(f"ë¬¸ì„œ ë°ì´í„°(ID-Chunk Map)ê°€ '{docs_pkl_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("âœ… ë²¡í„° DB ìƒì„±ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == '__main__':
    # 1. ë¬¸ì„œ ë¡œë“œ
    raw_documents = load_documents(DATA_PATH)
    
    # 2. ë¬¸ì„œê°€ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œëœ ê²½ìš°ì—ë§Œ ë‹¤ìŒ ë‹¨ê³„ ì§„í–‰
    if raw_documents:
        # 3. ë²•ë¥  ë¬¸ì„œ ì²­í‚¹
        legal_chunks = chunk_legal_documents(raw_documents)
        # 4. ë²¡í„° DB ìƒì„± ë° ì €ì¥
        create_and_save_vector_db(legal_chunks)
    else:
        print("âŒ ë¡œë“œí•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")